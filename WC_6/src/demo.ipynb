{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Expecting ',' delimiter: line 1 column 4954785 (char 4954784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dask Apply: 100%|██████████| 64/64 [00:05<00:00, 11.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "from swifter import set_defaults\n",
    "import swifter\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_defaults(progress_bar=True,\n",
    "             allow_dask_on_strings=True,\n",
    "             force_parallel=True,\n",
    "             npartitions=64)\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "\n",
    "output_path=Path.cwd().parents[1].joinpath('Output','Companies')\n",
    "\n",
    "full_df = pd.DataFrame()\n",
    "\n",
    "for company_dir in output_path.iterdir():\n",
    "    if company_dir.is_dir():\n",
    "        crawled_file = company_dir / \"crawled.json\"\n",
    "        if crawled_file.is_file():\n",
    "            try:\n",
    "                with open(str(crawled_file),'r') as f:\n",
    "                    data = json.load(f)\n",
    "                max_length = max(len(data['url']), len(data['html_lang']), len(data['html_string']))\n",
    "                df_data = {\n",
    "                        'Project': [data['Project']] * max_length,\n",
    "                        'url_base': [data['url_base']] * max_length,\n",
    "                        'url': data['url'] + [''] * (max_length - len(data['url'])),\n",
    "                        'html_lang': data['html_lang'] + [''] * (max_length - len(data['html_lang'])),\n",
    "                        'html_string': data['html_string'] + [''] * (max_length - len(data['html_string'])),\n",
    "                }\n",
    "                df = pd.DataFrame(df_data)\n",
    "                full_df = pd.concat([full_df, df], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "# Replace White Space with Null\n",
    "full_df.replace('', np.NaN, inplace=True)\n",
    "# Drop Null Values\n",
    "full_df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "def html_to_text(html: str):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        text = soup.text\n",
    "        list_of_string = soup.text.split(\"\\n\")   \n",
    "        final_text = [part_of_text.strip(\" \") for part_of_text in list_of_string if part_of_text]   \n",
    "\n",
    "        return final_text\n",
    "    except Exception:\n",
    "        #print(\"Error with html: \", html)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    full_df[\"text\"] = full_df.html_string.swifter.apply(lambda html: html_to_text(html))\n",
    "\n",
    "    full_df.drop(columns=\"html_string\", inplace=True)\n",
    "    \n",
    "full_df.to_parquet(\"../02_Output/New Input (IT)/Run 2023-03-01/combined data/text_dataframe.parquet\")\n",
    "ful_df.to_csv(\"../02_Output/New Input (IT)/Run 2023-03-01/combined data/text_dataframe.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
